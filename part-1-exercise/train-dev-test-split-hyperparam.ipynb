{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2:\n",
    "\n",
    "Do train-dev-test split and compute NLL loss on the three sets while training only on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Train-Dev-Test split (80-10-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def split_tensors(xs, ys, train_size=0.8, val_size=0.1, test_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split PyTorch tensors into train, validation, and test sets\n",
    "    \"\"\"\n",
    "    xs = torch.Tensor(xs).long()\n",
    "    ys = torch.Tensor(ys).long()\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(xs, ys)\n",
    "\n",
    "    # Calculate lengths for splits\n",
    "    total_size = len(dataset)\n",
    "    train_length = int(total_size * train_size)\n",
    "    val_length = int(total_size * val_size)\n",
    "    test_length = total_size - train_length - val_length\n",
    "\n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_length, val_length, test_length],\n",
    "        generator=torch.Generator().manual_seed(random_seed)\n",
    "    )\n",
    "\n",
    "    # Return the splits\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Create the bigram data and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../names.txt', 'r') as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(name for name in names))))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi, itos = {}, {}\n",
    "\n",
    "stoi['.'] = 0\n",
    "itos[0] = '.'\n",
    "\n",
    "for i, ch in enumerate(chars):\n",
    "    stoi[ch] = i + 1\n",
    "    itos[i + 1] = ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams: 228146\n"
     ]
    }
   ],
   "source": [
    "xs_bigram, ys_bigram, n = [], [], 0\n",
    "\n",
    "for name in names:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "\n",
    "    for ch1, ch2 in zip(name, name[1:]):\n",
    "        i1, i2 = stoi[ch1], stoi[ch2]\n",
    "        xs_bigram.append(i1)\n",
    "        ys_bigram.append(i2)\n",
    "        n += 1\n",
    "\n",
    "print(f\"Number of bigrams: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigram, val_bigram, test_bigram = split_tensors(xs_bigram, ys_bigram)\n",
    "\n",
    "X_train_bigram = torch.stack([x for x, _ in train_bigram])\n",
    "y_train_bigram = torch.stack([y for _, y in train_bigram])\n",
    "\n",
    "X_val_bigram = torch.stack([x for x, _ in val_bigram])\n",
    "y_val_bigram = torch.stack([y for _, y in val_bigram])\n",
    "\n",
    "X_test_bigram = torch.stack([x for x, _ in test_bigram])\n",
    "y_test_bigram = torch.stack([y for _, y in test_bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182516]), torch.Size([182516]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bigram.shape, y_train_bigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182516, 27]), torch.Size([22814, 27]), torch.Size([22816, 27]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bigram_enc = F.one_hot(X_train_bigram, num_classes=len(stoi)).float()\n",
    "X_val_bigram_enc = F.one_hot(X_val_bigram, num_classes=len(stoi)).float()\n",
    "X_test_bigram_enc = F.one_hot(X_test_bigram, num_classes=len(stoi)).float()\n",
    "\n",
    "X_train_bigram_enc.shape, X_val_bigram_enc.shape, X_test_bigram_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Train bigram model on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "bigramW = torch.randn((len(stoi), len(stoi)), generator=g, dtype=torch.float32, requires_grad=True)\n",
    "bigramW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 NLL: 3.758269786834717\n",
      "Epoch 2 NLL: 3.3710391521453857\n",
      "Epoch 3 NLL: 3.1540277004241943\n",
      "Epoch 4 NLL: 3.020277976989746\n",
      "Epoch 5 NLL: 2.9275436401367188\n",
      "Epoch 6 NLL: 2.860156774520874\n",
      "Epoch 7 NLL: 2.809420585632324\n",
      "Epoch 8 NLL: 2.769749879837036\n",
      "Epoch 9 NLL: 2.7376906871795654\n",
      "Epoch 10 NLL: 2.7110939025878906\n",
      "Epoch 11 NLL: 2.688584089279175\n",
      "Epoch 12 NLL: 2.6692535877227783\n",
      "Epoch 13 NLL: 2.6524770259857178\n",
      "Epoch 14 NLL: 2.637803554534912\n",
      "Epoch 15 NLL: 2.624891519546509\n",
      "Epoch 16 NLL: 2.613471508026123\n",
      "Epoch 17 NLL: 2.6033220291137695\n",
      "Epoch 18 NLL: 2.5942602157592773\n",
      "Epoch 19 NLL: 2.586132764816284\n",
      "Epoch 20 NLL: 2.5788097381591797\n",
      "Epoch 21 NLL: 2.5721826553344727\n",
      "Epoch 22 NLL: 2.566159725189209\n",
      "Epoch 23 NLL: 2.5606629848480225\n",
      "Epoch 24 NLL: 2.555626630783081\n",
      "Epoch 25 NLL: 2.5509958267211914\n",
      "Epoch 26 NLL: 2.546722888946533\n",
      "Epoch 27 NLL: 2.5427677631378174\n",
      "Epoch 28 NLL: 2.5390961170196533\n",
      "Epoch 29 NLL: 2.535680055618286\n",
      "Epoch 30 NLL: 2.532493829727173\n",
      "Epoch 31 NLL: 2.529515504837036\n",
      "Epoch 32 NLL: 2.5267271995544434\n",
      "Epoch 33 NLL: 2.5241119861602783\n",
      "Epoch 34 NLL: 2.521655559539795\n",
      "Epoch 35 NLL: 2.5193445682525635\n",
      "Epoch 36 NLL: 2.517167806625366\n",
      "Epoch 37 NLL: 2.5151147842407227\n",
      "Epoch 38 NLL: 2.513176679611206\n",
      "Epoch 39 NLL: 2.5113437175750732\n",
      "Epoch 40 NLL: 2.5096094608306885\n",
      "Epoch 41 NLL: 2.507965326309204\n",
      "Epoch 42 NLL: 2.506406307220459\n",
      "Epoch 43 NLL: 2.5049262046813965\n",
      "Epoch 44 NLL: 2.503518581390381\n",
      "Epoch 45 NLL: 2.5021793842315674\n",
      "Epoch 46 NLL: 2.500904083251953\n",
      "Epoch 47 NLL: 2.499687433242798\n",
      "Epoch 48 NLL: 2.4985268115997314\n",
      "Epoch 49 NLL: 2.4974184036254883\n",
      "Epoch 50 NLL: 2.4963583946228027\n",
      "Epoch 51 NLL: 2.4953441619873047\n",
      "Epoch 52 NLL: 2.494372844696045\n",
      "Epoch 53 NLL: 2.4934418201446533\n",
      "Epoch 54 NLL: 2.492548704147339\n",
      "Epoch 55 NLL: 2.4916911125183105\n",
      "Epoch 56 NLL: 2.4908673763275146\n",
      "Epoch 57 NLL: 2.4900753498077393\n",
      "Epoch 58 NLL: 2.4893131256103516\n",
      "Epoch 59 NLL: 2.488579273223877\n",
      "Epoch 60 NLL: 2.48787260055542\n",
      "Epoch 61 NLL: 2.4871909618377686\n",
      "Epoch 62 NLL: 2.4865336418151855\n",
      "Epoch 63 NLL: 2.4858992099761963\n",
      "Epoch 64 NLL: 2.485286235809326\n",
      "Epoch 65 NLL: 2.484693765640259\n",
      "Epoch 66 NLL: 2.484121322631836\n",
      "Epoch 67 NLL: 2.483567237854004\n",
      "Epoch 68 NLL: 2.4830307960510254\n",
      "Epoch 69 NLL: 2.482511520385742\n",
      "Epoch 70 NLL: 2.482008457183838\n",
      "Epoch 71 NLL: 2.481520414352417\n",
      "Epoch 72 NLL: 2.4810476303100586\n",
      "Epoch 73 NLL: 2.480588674545288\n",
      "Epoch 74 NLL: 2.4801430702209473\n",
      "Epoch 75 NLL: 2.4797110557556152\n",
      "Epoch 76 NLL: 2.479290723800659\n",
      "Epoch 77 NLL: 2.4788827896118164\n",
      "Epoch 78 NLL: 2.4784860610961914\n",
      "Epoch 79 NLL: 2.478100299835205\n",
      "Epoch 80 NLL: 2.477725028991699\n",
      "Epoch 81 NLL: 2.4773597717285156\n",
      "Epoch 82 NLL: 2.4770045280456543\n",
      "Epoch 83 NLL: 2.476658582687378\n",
      "Epoch 84 NLL: 2.4763216972351074\n",
      "Epoch 85 NLL: 2.4759929180145264\n",
      "Epoch 86 NLL: 2.475673198699951\n",
      "Epoch 87 NLL: 2.4753613471984863\n",
      "Epoch 88 NLL: 2.4750571250915527\n",
      "Epoch 89 NLL: 2.4747610092163086\n",
      "Epoch 90 NLL: 2.4744713306427\n",
      "Epoch 91 NLL: 2.474188804626465\n",
      "Epoch 92 NLL: 2.4739136695861816\n",
      "Epoch 93 NLL: 2.473644495010376\n",
      "Epoch 94 NLL: 2.473381757736206\n",
      "Epoch 95 NLL: 2.473125696182251\n",
      "Epoch 96 NLL: 2.472874879837036\n",
      "Epoch 97 NLL: 2.472630023956299\n",
      "Epoch 98 NLL: 2.47239089012146\n",
      "Epoch 99 NLL: 2.4721570014953613\n",
      "Epoch 100 NLL: 2.471928119659424\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    bigramW.grad = None\n",
    "\n",
    "    logits = X_train_bigram_enc @ bigramW\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(y_train_bigram)), y_train_bigram]\n",
    "\n",
    "    bigramNll = -torch.log(pred).mean()\n",
    "\n",
    "    bigramNll.backward()\n",
    "    with torch.no_grad():\n",
    "        bigramW -= 50*bigramW.grad\n",
    "\n",
    "    print(f\"Epoch {i+1} NLL: {bigramNll.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Compute NLL for val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nll_bigram(xenc, ys, W):\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(ys)), ys]\n",
    "\n",
    "    nll = -torch.log(pred).mean()\n",
    "\n",
    "    return nll.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.4763457775115967, 2.476706027984619)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nll, test_nll = compute_nll_bigram(X_val_bigram_enc, y_val_bigram, bigramW),\\\n",
    "      compute_nll_bigram(X_test_bigram_enc, y_test_bigram, bigramW)\n",
    "\n",
    "val_nll, test_nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Split trigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../names.txt', 'r') as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(name for name in names))))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi, itos, btoi, itob = {}, {}, {}, {}\n",
    "\n",
    "stoi['.'] = 0\n",
    "itos[0] = '.'\n",
    "\n",
    "for i, char in enumerate(chars):\n",
    "    stoi[char] = i + 1\n",
    "    itos[i + 1] = char\n",
    "\n",
    "cnt = 0\n",
    "btoi['..'] = 0\n",
    "itob[0] = '..'\n",
    "cnt += 1\n",
    "\n",
    "for ch1 in (['.'] + chars):\n",
    "    for ch2 in chars:\n",
    "        bigram = ch1 + ch2\n",
    "        btoi[bigram] = cnt\n",
    "        itob[cnt] = bigram\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of trigrams: 196113\n"
     ]
    }
   ],
   "source": [
    "xs_trigram, ys_trigram, n = [], [], 0\n",
    "\n",
    "for name in names:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        bigram = ch1 + ch2\n",
    "        i1, i2 = btoi[bigram], stoi[ch3]\n",
    "        xs_trigram.append(i1)\n",
    "        ys_trigram.append(i2)\n",
    "        n += 1\n",
    "\n",
    "print(f\"No of trigrams: {n}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_trigram, val_dataset_trigram, test_dataset_trigram = split_tensors(xs_trigram, ys_trigram)\n",
    "\n",
    "X_train_trigram = torch.stack([x for x, _ in train_dataset_trigram])\n",
    "y_train_trigram = torch.stack([y for _, y in train_dataset_trigram])\n",
    "\n",
    "X_val_trigram = torch.stack([x for x, _ in val_dataset_trigram])\n",
    "y_val_trigram = torch.stack([y for _, y in val_dataset_trigram])\n",
    "\n",
    "X_test_trigram = torch.stack([x for x, _ in test_dataset_trigram])\n",
    "y_test_trigram = torch.stack([y for _, y in test_dataset_trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([156890]), torch.Size([156890]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trigram.shape, y_train_trigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([19611]), torch.Size([19611]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_trigram.shape, y_val_trigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([19612]), torch.Size([19612]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_trigram.shape, y_test_trigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([156890, 703]), torch.Size([19611, 703]), torch.Size([19612, 703]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trigram_enc = F.one_hot(X_train_trigram, num_classes=len(btoi)).float()\n",
    "X_val_trigram_enc = F.one_hot(X_val_trigram, num_classes=len(btoi)).float()\n",
    "X_test_trigram_enc = F.one_hot(X_test_trigram, num_classes=len(btoi)).float()\n",
    "\n",
    "X_train_trigram_enc.shape, X_val_trigram_enc.shape, X_test_trigram_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Train trigram model on `train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([703, 27])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "trigramW = torch.randn((len(btoi), len(stoi)), generator=g, dtype=torch.float32, requires_grad=True)\n",
    "trigramW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 NLL: 3.7393720149993896\n",
      "Epoch 2 NLL: 3.6250696182250977\n",
      "Epoch 3 NLL: 3.521294355392456\n",
      "Epoch 4 NLL: 3.428551435470581\n",
      "Epoch 5 NLL: 3.3469762802124023\n",
      "Epoch 6 NLL: 3.2757656574249268\n",
      "Epoch 7 NLL: 3.2133870124816895\n",
      "Epoch 8 NLL: 3.158212661743164\n",
      "Epoch 9 NLL: 3.108926773071289\n",
      "Epoch 10 NLL: 3.064568281173706\n",
      "Epoch 11 NLL: 3.0244274139404297\n",
      "Epoch 12 NLL: 2.987946033477783\n",
      "Epoch 13 NLL: 2.954658269882202\n",
      "Epoch 14 NLL: 2.924161195755005\n",
      "Epoch 15 NLL: 2.8961007595062256\n",
      "Epoch 16 NLL: 2.8701677322387695\n",
      "Epoch 17 NLL: 2.846097469329834\n",
      "Epoch 18 NLL: 2.8236639499664307\n",
      "Epoch 19 NLL: 2.8026764392852783\n",
      "Epoch 20 NLL: 2.782975196838379\n",
      "Epoch 21 NLL: 2.764425277709961\n",
      "Epoch 22 NLL: 2.7469122409820557\n",
      "Epoch 23 NLL: 2.730339288711548\n",
      "Epoch 24 NLL: 2.7146220207214355\n",
      "Epoch 25 NLL: 2.6996874809265137\n",
      "Epoch 26 NLL: 2.6854729652404785\n",
      "Epoch 27 NLL: 2.671922206878662\n",
      "Epoch 28 NLL: 2.6589865684509277\n",
      "Epoch 29 NLL: 2.6466221809387207\n",
      "Epoch 30 NLL: 2.6347899436950684\n",
      "Epoch 31 NLL: 2.6234548091888428\n",
      "Epoch 32 NLL: 2.6125853061676025\n",
      "Epoch 33 NLL: 2.6021528244018555\n",
      "Epoch 34 NLL: 2.5921308994293213\n",
      "Epoch 35 NLL: 2.5824954509735107\n",
      "Epoch 36 NLL: 2.5732247829437256\n",
      "Epoch 37 NLL: 2.564298152923584\n",
      "Epoch 38 NLL: 2.555696725845337\n",
      "Epoch 39 NLL: 2.547402858734131\n",
      "Epoch 40 NLL: 2.539400339126587\n",
      "Epoch 41 NLL: 2.5316741466522217\n",
      "Epoch 42 NLL: 2.524209976196289\n",
      "Epoch 43 NLL: 2.5169949531555176\n",
      "Epoch 44 NLL: 2.510016679763794\n",
      "Epoch 45 NLL: 2.503262996673584\n",
      "Epoch 46 NLL: 2.4967241287231445\n",
      "Epoch 47 NLL: 2.490389347076416\n",
      "Epoch 48 NLL: 2.4842498302459717\n",
      "Epoch 49 NLL: 2.4782960414886475\n",
      "Epoch 50 NLL: 2.472520112991333\n",
      "Epoch 51 NLL: 2.466913938522339\n",
      "Epoch 52 NLL: 2.461470365524292\n",
      "Epoch 53 NLL: 2.4561822414398193\n",
      "Epoch 54 NLL: 2.451042890548706\n",
      "Epoch 55 NLL: 2.4460461139678955\n",
      "Epoch 56 NLL: 2.4411861896514893\n",
      "Epoch 57 NLL: 2.4364571571350098\n",
      "Epoch 58 NLL: 2.431854009628296\n",
      "Epoch 59 NLL: 2.4273717403411865\n",
      "Epoch 60 NLL: 2.4230051040649414\n",
      "Epoch 61 NLL: 2.418750047683716\n",
      "Epoch 62 NLL: 2.414602279663086\n",
      "Epoch 63 NLL: 2.410557270050049\n",
      "Epoch 64 NLL: 2.406611204147339\n",
      "Epoch 65 NLL: 2.4027605056762695\n",
      "Epoch 66 NLL: 2.3990015983581543\n",
      "Epoch 67 NLL: 2.3953309059143066\n",
      "Epoch 68 NLL: 2.3917455673217773\n",
      "Epoch 69 NLL: 2.388242244720459\n",
      "Epoch 70 NLL: 2.3848180770874023\n",
      "Epoch 71 NLL: 2.381469964981079\n",
      "Epoch 72 NLL: 2.3781957626342773\n",
      "Epoch 73 NLL: 2.374992847442627\n",
      "Epoch 74 NLL: 2.3718581199645996\n",
      "Epoch 75 NLL: 2.3687901496887207\n",
      "Epoch 76 NLL: 2.365786075592041\n",
      "Epoch 77 NLL: 2.362844467163086\n",
      "Epoch 78 NLL: 2.3599627017974854\n",
      "Epoch 79 NLL: 2.3571388721466064\n",
      "Epoch 80 NLL: 2.3543713092803955\n",
      "Epoch 81 NLL: 2.351658582687378\n",
      "Epoch 82 NLL: 2.348998546600342\n",
      "Epoch 83 NLL: 2.3463892936706543\n",
      "Epoch 84 NLL: 2.343829870223999\n",
      "Epoch 85 NLL: 2.3413188457489014\n",
      "Epoch 86 NLL: 2.3388545513153076\n",
      "Epoch 87 NLL: 2.336435556411743\n",
      "Epoch 88 NLL: 2.3340604305267334\n",
      "Epoch 89 NLL: 2.331728458404541\n",
      "Epoch 90 NLL: 2.329437732696533\n",
      "Epoch 91 NLL: 2.3271877765655518\n",
      "Epoch 92 NLL: 2.324977159500122\n",
      "Epoch 93 NLL: 2.3228046894073486\n",
      "Epoch 94 NLL: 2.320669412612915\n",
      "Epoch 95 NLL: 2.318570852279663\n",
      "Epoch 96 NLL: 2.31650710105896\n",
      "Epoch 97 NLL: 2.3144776821136475\n",
      "Epoch 98 NLL: 2.3124821186065674\n",
      "Epoch 99 NLL: 2.310518980026245\n",
      "Epoch 100 NLL: 2.3085877895355225\n",
      "Epoch 101 NLL: 2.306687355041504\n",
      "Epoch 102 NLL: 2.3048176765441895\n",
      "Epoch 103 NLL: 2.3029773235321045\n",
      "Epoch 104 NLL: 2.3011655807495117\n",
      "Epoch 105 NLL: 2.299382448196411\n",
      "Epoch 106 NLL: 2.297626495361328\n",
      "Epoch 107 NLL: 2.2958974838256836\n",
      "Epoch 108 NLL: 2.294194221496582\n",
      "Epoch 109 NLL: 2.2925167083740234\n",
      "Epoch 110 NLL: 2.2908647060394287\n",
      "Epoch 111 NLL: 2.289236545562744\n",
      "Epoch 112 NLL: 2.2876322269439697\n",
      "Epoch 113 NLL: 2.2860512733459473\n",
      "Epoch 114 NLL: 2.2844932079315186\n",
      "Epoch 115 NLL: 2.2829575538635254\n",
      "Epoch 116 NLL: 2.2814435958862305\n",
      "Epoch 117 NLL: 2.2799510955810547\n",
      "Epoch 118 NLL: 2.2784793376922607\n",
      "Epoch 119 NLL: 2.2770280838012695\n",
      "Epoch 120 NLL: 2.275596857070923\n",
      "Epoch 121 NLL: 2.2741849422454834\n",
      "Epoch 122 NLL: 2.272792339324951\n",
      "Epoch 123 NLL: 2.271418571472168\n",
      "Epoch 124 NLL: 2.2700634002685547\n",
      "Epoch 125 NLL: 2.268725633621216\n",
      "Epoch 126 NLL: 2.2674059867858887\n",
      "Epoch 127 NLL: 2.266103744506836\n",
      "Epoch 128 NLL: 2.264817714691162\n",
      "Epoch 129 NLL: 2.2635488510131836\n",
      "Epoch 130 NLL: 2.262296199798584\n",
      "Epoch 131 NLL: 2.261059522628784\n",
      "Epoch 132 NLL: 2.259838342666626\n",
      "Epoch 133 NLL: 2.2586326599121094\n",
      "Epoch 134 NLL: 2.257441759109497\n",
      "Epoch 135 NLL: 2.256265640258789\n",
      "Epoch 136 NLL: 2.2551043033599854\n",
      "Epoch 137 NLL: 2.2539570331573486\n",
      "Epoch 138 NLL: 2.2528233528137207\n",
      "Epoch 139 NLL: 2.2517037391662598\n",
      "Epoch 140 NLL: 2.2505974769592285\n",
      "Epoch 141 NLL: 2.249504327774048\n",
      "Epoch 142 NLL: 2.2484240531921387\n",
      "Epoch 143 NLL: 2.24735689163208\n",
      "Epoch 144 NLL: 2.2463018894195557\n",
      "Epoch 145 NLL: 2.2452590465545654\n",
      "Epoch 146 NLL: 2.2442283630371094\n",
      "Epoch 147 NLL: 2.2432093620300293\n",
      "Epoch 148 NLL: 2.2422022819519043\n",
      "Epoch 149 NLL: 2.241206407546997\n",
      "Epoch 150 NLL: 2.2402217388153076\n",
      "Epoch 151 NLL: 2.239248037338257\n",
      "Epoch 152 NLL: 2.2382853031158447\n",
      "Epoch 153 NLL: 2.2373335361480713\n",
      "Epoch 154 NLL: 2.23639178276062\n",
      "Epoch 155 NLL: 2.2354602813720703\n",
      "Epoch 156 NLL: 2.234539270401001\n",
      "Epoch 157 NLL: 2.233628034591675\n",
      "Epoch 158 NLL: 2.2327263355255127\n",
      "Epoch 159 NLL: 2.231834650039673\n",
      "Epoch 160 NLL: 2.230952501296997\n",
      "Epoch 161 NLL: 2.2300796508789062\n",
      "Epoch 162 NLL: 2.229215621948242\n",
      "Epoch 163 NLL: 2.228361129760742\n",
      "Epoch 164 NLL: 2.22751522064209\n",
      "Epoch 165 NLL: 2.2266781330108643\n",
      "Epoch 166 NLL: 2.2258496284484863\n",
      "Epoch 167 NLL: 2.225029468536377\n",
      "Epoch 168 NLL: 2.2242181301116943\n",
      "Epoch 169 NLL: 2.223414421081543\n",
      "Epoch 170 NLL: 2.22261905670166\n",
      "Epoch 171 NLL: 2.221831798553467\n",
      "Epoch 172 NLL: 2.221052408218384\n",
      "Epoch 173 NLL: 2.220280647277832\n",
      "Epoch 174 NLL: 2.2195165157318115\n",
      "Epoch 175 NLL: 2.2187600135803223\n",
      "Epoch 176 NLL: 2.218010663986206\n",
      "Epoch 177 NLL: 2.217268943786621\n",
      "Epoch 178 NLL: 2.216533899307251\n",
      "Epoch 179 NLL: 2.215806245803833\n",
      "Epoch 180 NLL: 2.215085506439209\n",
      "Epoch 181 NLL: 2.2143714427948\n",
      "Epoch 182 NLL: 2.2136645317077637\n",
      "Epoch 183 NLL: 2.2129640579223633\n",
      "Epoch 184 NLL: 2.2122702598571777\n",
      "Epoch 185 NLL: 2.211582899093628\n",
      "Epoch 186 NLL: 2.2109017372131348\n",
      "Epoch 187 NLL: 2.2102270126342773\n",
      "Epoch 188 NLL: 2.2095589637756348\n",
      "Epoch 189 NLL: 2.2088968753814697\n",
      "Epoch 190 NLL: 2.208240270614624\n",
      "Epoch 191 NLL: 2.207590103149414\n",
      "Epoch 192 NLL: 2.2069458961486816\n",
      "Epoch 193 NLL: 2.2063071727752686\n",
      "Epoch 194 NLL: 2.205674648284912\n",
      "Epoch 195 NLL: 2.205047369003296\n",
      "Epoch 196 NLL: 2.204425573348999\n",
      "Epoch 197 NLL: 2.2038094997406006\n",
      "Epoch 198 NLL: 2.2031986713409424\n",
      "Epoch 199 NLL: 2.2025930881500244\n",
      "Epoch 200 NLL: 2.201993465423584\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    trigramW.grad = None\n",
    "\n",
    "    logits = X_train_trigram_enc @ trigramW\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(y_train_trigram)), y_train_trigram]\n",
    "\n",
    "    trigramNll = -torch.log(pred).mean()\n",
    "\n",
    "    trigramNll.backward()\n",
    "    with torch.no_grad():\n",
    "        trigramW -= 75*trigramW.grad\n",
    "\n",
    "    print(f\"Epoch {i+1} NLL: {trigramNll.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 : Get NLL for `val` and `test` split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll(xenc, ys, W):\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(ys)), ys]\n",
    "\n",
    "    nll = -torch.log(pred).mean().item()\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.2296581268310547, 2.2140655517578125)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nll = get_nll(X_val_trigram_enc, y_val_trigram, trigramW)\n",
    "test_nll = get_nll(X_test_trigram_enc, y_test_trigram, trigramW)\n",
    "val_nll, test_nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Analyse the regularization hyperparameter and choose the best one for bigram and trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Create the loop for L2 normalization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization param: 1, Train loss: 3.1852142810821533, Val loss: 3.1824824810028076\n",
      "Regularization param: 0.5, Train loss: 2.947974681854248, Val loss: 2.944690704345703\n",
      "Regularization param: 0.1, Train loss: 2.5745139122009277, Val loss: 2.5758018493652344\n",
      "Regularization param: 0.05, Train loss: 2.508540153503418, Val loss: 2.5116376876831055\n",
      "Regularization param: 0.01, Train loss: 2.4907517433166504, Val loss: 2.495285987854004\n",
      "Regularization param: 0.005, Train loss: 2.4907517433166504, Val loss: 2.495285987854004\n",
      "Regularization param: 0.001, Train loss: 2.4907517433166504, Val loss: 2.495285987854004\n"
     ]
    }
   ],
   "source": [
    "for reg_param in [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    bigramW = torch.randn((len(stoi), len(stoi)), generator=g, dtype=torch.float32, requires_grad=True)\n",
    "  \n",
    "    ## adjust lr since there is no convergence in the higher reg_param for very high lr\n",
    "    lr = 1/reg_param\n",
    "    if lr > 100:\n",
    "        lr = 100\n",
    "    elif lr < 0.01:\n",
    "        lr = 0.01\n",
    "        \n",
    "    for epoch in range(100):\n",
    "        bigramW.grad = None\n",
    "\n",
    "        ## Train \n",
    "        logits = X_train_bigram_enc @ bigramW\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_train_bigram)), y_train_bigram]\n",
    "\n",
    "        bigramNll = -torch.log(pred).mean() + reg_param * (((bigramW.data)**2).mean())\n",
    "\n",
    "        bigramNll.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bigramW -= lr*bigramW.grad\n",
    "\n",
    "    ## Train and Val losses\n",
    "    with torch.no_grad():\n",
    "        logits = X_train_bigram_enc @ bigramW\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_train_bigram)), y_train_bigram]\n",
    "\n",
    "        bigramNll_train = -torch.log(pred).mean()\n",
    "\n",
    "        logits = X_val_bigram_enc @ bigramW\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_val_bigram)), y_val_bigram]\n",
    "\n",
    "        bigramNll_val = -torch.log(pred).mean()\n",
    "\n",
    "    print(f\"Regularization param: {reg_param}, Train loss: {bigramNll_train}, Val loss: {bigramNll_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Take best L2 parameters and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train NLL: 3.1295087337493896, Val NLL: 3.1260931491851807\n",
      "Epoch 2, Train NLL: 2.9141311645507812, Val NLL: 2.910109758377075\n",
      "Epoch 3, Train NLL: 2.7997078895568848, Val NLL: 2.7955095767974854\n",
      "Epoch 4, Train NLL: 2.7334985733032227, Val NLL: 2.7308766841888428\n",
      "Epoch 5, Train NLL: 2.686508893966675, Val NLL: 2.684903144836426\n",
      "Epoch 6, Train NLL: 2.6544017791748047, Val NLL: 2.653481960296631\n",
      "Epoch 7, Train NLL: 2.6291565895080566, Val NLL: 2.6294138431549072\n",
      "Epoch 8, Train NLL: 2.617499351501465, Val NLL: 2.617274045944214\n",
      "Epoch 9, Train NLL: 2.605008840560913, Val NLL: 2.606895685195923\n",
      "Epoch 10, Train NLL: 2.614349842071533, Val NLL: 2.6142373085021973\n",
      "Epoch 11, Train NLL: 2.582610845565796, Val NLL: 2.5848169326782227\n",
      "Epoch 12, Train NLL: 2.577045202255249, Val NLL: 2.577923059463501\n",
      "Epoch 13, Train NLL: 2.5659255981445312, Val NLL: 2.5690176486968994\n",
      "Epoch 14, Train NLL: 2.578768014907837, Val NLL: 2.5796923637390137\n",
      "Epoch 15, Train NLL: 2.551987648010254, Val NLL: 2.555061101913452\n",
      "Epoch 16, Train NLL: 2.552476406097412, Val NLL: 2.554110527038574\n",
      "Epoch 17, Train NLL: 2.544266939163208, Val NLL: 2.548006534576416\n",
      "Epoch 18, Train NLL: 2.5591602325439453, Val NLL: 2.5606534481048584\n",
      "Epoch 19, Train NLL: 2.5341603755950928, Val NLL: 2.537726402282715\n",
      "Epoch 20, Train NLL: 2.537140130996704, Val NLL: 2.5392377376556396\n",
      "Epoch 21, Train NLL: 2.530712366104126, Val NLL: 2.5348620414733887\n",
      "Epoch 22, Train NLL: 2.5469894409179688, Val NLL: 2.548847198486328\n",
      "Epoch 23, Train NLL: 2.5227386951446533, Val NLL: 2.5266306400299072\n",
      "Epoch 24, Train NLL: 2.526832342147827, Val NLL: 2.52925443649292\n",
      "Epoch 25, Train NLL: 2.5216221809387207, Val NLL: 2.5260677337646484\n",
      "Epoch 26, Train NLL: 2.538877487182617, Val NLL: 2.5410025119781494\n",
      "Epoch 27, Train NLL: 2.5149152278900146, Val NLL: 2.519052743911743\n",
      "Epoch 28, Train NLL: 2.519512176513672, Val NLL: 2.522190570831299\n",
      "Epoch 29, Train NLL: 2.5151660442352295, Val NLL: 2.519850015640259\n",
      "Epoch 30, Train NLL: 2.5331292152404785, Val NLL: 2.5354743003845215\n",
      "Epoch 31, Train NLL: 2.509253740310669, Val NLL: 2.513599395751953\n",
      "Epoch 32, Train NLL: 2.5140812397003174, Val NLL: 2.5169806480407715\n",
      "Epoch 33, Train NLL: 2.510357618331909, Val NLL: 2.5152511596679688\n",
      "Epoch 34, Train NLL: 2.528838634490967, Val NLL: 2.5313808917999268\n",
      "Epoch 35, Train NLL: 2.504974603652954, Val NLL: 2.509509563446045\n",
      "Epoch 36, Train NLL: 2.509913206100464, Val NLL: 2.513012647628784\n",
      "Epoch 37, Train NLL: 2.5066416263580322, Val NLL: 2.5117263793945312\n",
      "Epoch 38, Train NLL: 2.5255038738250732, Val NLL: 2.5282280445098877\n",
      "Epoch 39, Train NLL: 2.5016329288482666, Val NLL: 2.5063438415527344\n",
      "Epoch 40, Train NLL: 2.5066328048706055, Val NLL: 2.509916067123413\n",
      "Epoch 41, Train NLL: 2.503688335418701, Val NLL: 2.5089499950408936\n",
      "Epoch 42, Train NLL: 2.5228328704833984, Val NLL: 2.5257256031036377\n",
      "Epoch 43, Train NLL: 2.4989569187164307, Val NLL: 2.503830909729004\n",
      "Epoch 44, Train NLL: 2.503999710083008, Val NLL: 2.507451295852661\n",
      "Epoch 45, Train NLL: 2.5012922286987305, Val NLL: 2.5067150592803955\n",
      "Epoch 46, Train NLL: 2.5206456184387207, Val NLL: 2.5236942768096924\n",
      "Epoch 47, Train NLL: 2.496772527694702, Val NLL: 2.5017967224121094\n",
      "Epoch 48, Train NLL: 2.5018529891967773, Val NLL: 2.5054566860198975\n",
      "Epoch 49, Train NLL: 2.499316453933716, Val NLL: 2.504885673522949\n",
      "Epoch 50, Train NLL: 2.518826484680176, Val NLL: 2.5220160484313965\n",
      "Epoch 51, Train NLL: 2.4949612617492676, Val NLL: 2.500122308731079\n",
      "Epoch 52, Train NLL: 2.5000762939453125, Val NLL: 2.503816604614258\n",
      "Epoch 53, Train NLL: 2.4976649284362793, Val NLL: 2.503365993499756\n",
      "Epoch 54, Train NLL: 2.5172934532165527, Val NLL: 2.5206100940704346\n",
      "Epoch 55, Train NLL: 2.493439197540283, Val NLL: 2.4987223148345947\n",
      "Epoch 56, Train NLL: 2.4985859394073486, Val NLL: 2.502448797225952\n",
      "Epoch 57, Train NLL: 2.496267795562744, Val NLL: 2.5020864009857178\n",
      "Epoch 58, Train NLL: 2.5159876346588135, Val NLL: 2.5194177627563477\n",
      "Epoch 59, Train NLL: 2.4921457767486572, Val NLL: 2.4975380897521973\n",
      "Epoch 60, Train NLL: 2.4973220825195312, Val NLL: 2.5012927055358887\n",
      "Epoch 61, Train NLL: 2.4950733184814453, Val NLL: 2.5009961128234863\n",
      "Epoch 62, Train NLL: 2.5148637294769287, Val NLL: 2.5183944702148438\n",
      "Epoch 63, Train NLL: 2.491034746170044, Val NLL: 2.4965240955352783\n",
      "Epoch 64, Train NLL: 2.4962382316589355, Val NLL: 2.500304937362671\n",
      "Epoch 65, Train NLL: 2.494042158126831, Val NLL: 2.5000569820404053\n",
      "Epoch 66, Train NLL: 2.513888120651245, Val NLL: 2.517508029937744\n",
      "Epoch 67, Train NLL: 2.4900715351104736, Val NLL: 2.4956469535827637\n",
      "Epoch 68, Train NLL: 2.4952993392944336, Val NLL: 2.499450445175171\n",
      "Epoch 69, Train NLL: 2.4931447505950928, Val NLL: 2.4992408752441406\n",
      "Epoch 70, Train NLL: 2.513036012649536, Val NLL: 2.5167338848114014\n",
      "Epoch 71, Train NLL: 2.489229440689087, Val NLL: 2.494880199432373\n",
      "Epoch 72, Train NLL: 2.494478464126587, Val NLL: 2.4987032413482666\n",
      "Epoch 73, Train NLL: 2.4923572540283203, Val NLL: 2.4985239505767822\n",
      "Epoch 74, Train NLL: 2.512284994125366, Val NLL: 2.5160512924194336\n",
      "Epoch 75, Train NLL: 2.488487720489502, Val NLL: 2.49420428276062\n",
      "Epoch 76, Train NLL: 2.493755578994751, Val NLL: 2.498044967651367\n",
      "Epoch 77, Train NLL: 2.4916610717773438, Val NLL: 2.497889518737793\n",
      "Epoch 78, Train NLL: 2.5116188526153564, Val NLL: 2.5154454708099365\n",
      "Epoch 79, Train NLL: 2.4878296852111816, Val NLL: 2.4936041831970215\n",
      "Epoch 80, Train NLL: 2.4931142330169678, Val NLL: 2.497459650039673\n",
      "Epoch 81, Train NLL: 2.4910404682159424, Val NLL: 2.497323513031006\n",
      "Epoch 82, Train NLL: 2.511023998260498, Val NLL: 2.5149030685424805\n",
      "Epoch 83, Train NLL: 2.4872424602508545, Val NLL: 2.4930672645568848\n",
      "Epoch 84, Train NLL: 2.492541551589966, Val NLL: 2.496936082839966\n",
      "Epoch 85, Train NLL: 2.490485191345215, Val NLL: 2.4968152046203613\n",
      "Epoch 86, Train NLL: 2.510490655899048, Val NLL: 2.5144150257110596\n",
      "Epoch 87, Train NLL: 2.4867148399353027, Val NLL: 2.4925835132598877\n",
      "Epoch 88, Train NLL: 2.4920263290405273, Val NLL: 2.4964640140533447\n",
      "Epoch 89, Train NLL: 2.489985227584839, Val NLL: 2.496356248855591\n",
      "Epoch 90, Train NLL: 2.5100090503692627, Val NLL: 2.5139729976654053\n",
      "Epoch 91, Train NLL: 2.4862382411956787, Val NLL: 2.4921445846557617\n",
      "Epoch 92, Train NLL: 2.491560935974121, Val NLL: 2.49603533744812\n",
      "Epoch 93, Train NLL: 2.489532232284546, Val NLL: 2.495939016342163\n",
      "Epoch 94, Train NLL: 2.5095717906951904, Val NLL: 2.5135700702667236\n",
      "Epoch 95, Train NLL: 2.4858052730560303, Val NLL: 2.4917447566986084\n",
      "Epoch 96, Train NLL: 2.491137981414795, Val NLL: 2.4956445693969727\n",
      "Epoch 97, Train NLL: 2.4891202449798584, Val NLL: 2.4955575466156006\n",
      "Epoch 98, Train NLL: 2.5091733932495117, Val NLL: 2.5132014751434326\n",
      "Epoch 99, Train NLL: 2.485410690307617, Val NLL: 2.4913783073425293\n",
      "Epoch 100, Train NLL: 2.4907517433166504, Val NLL: 2.495285987854004\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "bigramW_best = torch.randn((len(stoi), len(stoi)), generator=g, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "reg_param = 0.01\n",
    "lr = 100  \n",
    "\n",
    "for epoch in range(100):\n",
    "    bigramW_best.grad = None\n",
    "\n",
    "    ## Train \n",
    "    logits = X_train_bigram_enc @ bigramW_best\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(y_train_bigram)), y_train_bigram]\n",
    "\n",
    "    bigramNll = -torch.log(pred).mean() + reg_param * (((bigramW_best.data)**2).mean())\n",
    "\n",
    "    bigramNll.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bigramW_best -= lr*bigramW_best.grad\n",
    "\n",
    "    ## Train and Val losses\n",
    "    with torch.no_grad():\n",
    "        logits = X_train_bigram_enc @ bigramW_best\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_train_bigram)), y_train_bigram]\n",
    "\n",
    "        bigramNll_train = -torch.log(pred).mean().item()\n",
    "\n",
    "        logits = X_val_bigram_enc @ bigramW_best\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_val_bigram)), y_val_bigram]\n",
    "\n",
    "        bigramNll_val = -torch.log(pred).mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train NLL: {bigramNll_train}, Val NLL: {bigramNll_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Get the test NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4967715740203857"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = X_test_bigram_enc @ bigramW_best\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(y_test_bigram)), y_test_bigram]\n",
    "\n",
    "    bigramNll_test = -torch.log(pred).mean().item()\n",
    "\n",
    "bigramNll_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Create the loop for L2 normalization parameter trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization param: 1, Train loss: 3.5922605991363525, Val loss: 3.6011605262756348\n",
      "Regularization param: 0.5, Train loss: 3.4635202884674072, Val loss: 3.472970724105835\n",
      "Regularization param: 0.1, Train loss: 2.9193384647369385, Val loss: 2.9296774864196777\n",
      "Regularization param: 0.05, Train loss: 2.6655161380767822, Val loss: 2.67805814743042\n",
      "Regularization param: 0.01, Train loss: 2.2568893432617188, Val loss: 2.2825212478637695\n",
      "Regularization param: 0.005, Train loss: 2.2568893432617188, Val loss: 2.2825212478637695\n",
      "Regularization param: 0.001, Train loss: 2.2568893432617188, Val loss: 2.2825212478637695\n"
     ]
    }
   ],
   "source": [
    "for reg_param in [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    trigramW = torch.randn((len(btoi), len(stoi)), generator=g, dtype=torch.float32, requires_grad=True)\n",
    "  \n",
    "    ## adjust lr since there is no convergence in the higher reg_param for very high lr\n",
    "    lr = 1/reg_param\n",
    "    if lr > 100:\n",
    "        lr = 100\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        trigramW.grad = None\n",
    "\n",
    "        ## Train \n",
    "        logits = X_train_trigram_enc @ trigramW\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_train_trigram)), y_train_trigram]\n",
    "\n",
    "        trigramNll = -torch.log(pred).mean() + reg_param * (((trigramW.data)**2).mean())\n",
    "\n",
    "        trigramNll.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            trigramW -= lr*trigramW.grad\n",
    "\n",
    "    ## Train and Val losses\n",
    "    with torch.no_grad():\n",
    "        logits = X_train_trigram_enc @ trigramW\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_train_trigram)), y_train_trigram]\n",
    "\n",
    "        trigramNll_train = -torch.log(pred).mean()\n",
    "\n",
    "        logits = X_val_trigram_enc @ trigramW\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_val_trigram)), y_val_trigram]\n",
    "\n",
    "        trigramNll_val = -torch.log(pred).mean()\n",
    "\n",
    "    print(f\"Regularization param: {reg_param}, Train loss: {trigramNll_train}, Val loss: {trigramNll_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Take best L2 parameters and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train NLL: 3.5882608890533447, Val NLL: 3.5972273349761963\n",
      "Epoch 2, Train NLL: 3.4563732147216797, Val NLL: 3.4659299850463867\n",
      "Epoch 3, Train NLL: 3.3446061611175537, Val NLL: 3.3545033931732178\n",
      "Epoch 4, Train NLL: 3.2515594959259033, Val NLL: 3.2616055011749268\n",
      "Epoch 5, Train NLL: 3.173459768295288, Val NLL: 3.1835429668426514\n",
      "Epoch 6, Train NLL: 3.106626510620117, Val NLL: 3.1167030334472656\n",
      "Epoch 7, Train NLL: 3.048572301864624, Val NLL: 3.058645725250244\n",
      "Epoch 8, Train NLL: 2.997666835784912, Val NLL: 3.007766008377075\n",
      "Epoch 9, Train NLL: 2.9527089595794678, Val NLL: 2.9628734588623047\n",
      "Epoch 10, Train NLL: 2.912720203399658, Val NLL: 2.9229929447174072\n",
      "Epoch 11, Train NLL: 2.876875638961792, Val NLL: 2.887296438217163\n",
      "Epoch 12, Train NLL: 2.8444905281066895, Val NLL: 2.855093240737915\n",
      "Epoch 13, Train NLL: 2.8150100708007812, Val NLL: 2.8258211612701416\n",
      "Epoch 14, Train NLL: 2.7879936695098877, Val NLL: 2.799031972885132\n",
      "Epoch 15, Train NLL: 2.763092517852783, Val NLL: 2.7743709087371826\n",
      "Epoch 16, Train NLL: 2.740028142929077, Val NLL: 2.751555919647217\n",
      "Epoch 17, Train NLL: 2.718576431274414, Val NLL: 2.730358123779297\n",
      "Epoch 18, Train NLL: 2.698552370071411, Val NLL: 2.7105929851531982\n",
      "Epoch 19, Train NLL: 2.6798033714294434, Val NLL: 2.692105293273926\n",
      "Epoch 20, Train NLL: 2.6622002124786377, Val NLL: 2.6747663021087646\n",
      "Epoch 21, Train NLL: 2.6456339359283447, Val NLL: 2.65846586227417\n",
      "Epoch 22, Train NLL: 2.6300103664398193, Val NLL: 2.643110513687134\n",
      "Epoch 23, Train NLL: 2.615248680114746, Val NLL: 2.628618001937866\n",
      "Epoch 24, Train NLL: 2.6012775897979736, Val NLL: 2.614917278289795\n",
      "Epoch 25, Train NLL: 2.5880346298217773, Val NLL: 2.601945400238037\n",
      "Epoch 26, Train NLL: 2.5754637718200684, Val NLL: 2.5896458625793457\n",
      "Epoch 27, Train NLL: 2.5635149478912354, Val NLL: 2.5779683589935303\n",
      "Epoch 28, Train NLL: 2.552142381668091, Val NLL: 2.566866636276245\n",
      "Epoch 29, Train NLL: 2.5413057804107666, Val NLL: 2.5562992095947266\n",
      "Epoch 30, Train NLL: 2.530967950820923, Val NLL: 2.546229124069214\n",
      "Epoch 31, Train NLL: 2.521094799041748, Val NLL: 2.5366220474243164\n",
      "Epoch 32, Train NLL: 2.511655569076538, Val NLL: 2.5274460315704346\n",
      "Epoch 33, Train NLL: 2.502622365951538, Val NLL: 2.5186734199523926\n",
      "Epoch 34, Train NLL: 2.493969678878784, Val NLL: 2.5102782249450684\n",
      "Epoch 35, Train NLL: 2.485673427581787, Val NLL: 2.5022354125976562\n",
      "Epoch 36, Train NLL: 2.477712392807007, Val NLL: 2.4945249557495117\n",
      "Epoch 37, Train NLL: 2.470066547393799, Val NLL: 2.4871251583099365\n",
      "Epoch 38, Train NLL: 2.4627175331115723, Val NLL: 2.480018377304077\n",
      "Epoch 39, Train NLL: 2.455648183822632, Val NLL: 2.473187208175659\n",
      "Epoch 40, Train NLL: 2.4488425254821777, Val NLL: 2.466615915298462\n",
      "Epoch 41, Train NLL: 2.442286729812622, Val NLL: 2.460289716720581\n",
      "Epoch 42, Train NLL: 2.4359664916992188, Val NLL: 2.4541945457458496\n",
      "Epoch 43, Train NLL: 2.4298691749572754, Val NLL: 2.4483184814453125\n",
      "Epoch 44, Train NLL: 2.423983335494995, Val NLL: 2.4426493644714355\n",
      "Epoch 45, Train NLL: 2.41829776763916, Val NLL: 2.437175750732422\n",
      "Epoch 46, Train NLL: 2.412802219390869, Val NLL: 2.4318881034851074\n",
      "Epoch 47, Train NLL: 2.4074864387512207, Val NLL: 2.4267756938934326\n",
      "Epoch 48, Train NLL: 2.4023423194885254, Val NLL: 2.421830892562866\n",
      "Epoch 49, Train NLL: 2.3973608016967773, Val NLL: 2.417044162750244\n",
      "Epoch 50, Train NLL: 2.392533779144287, Val NLL: 2.4124081134796143\n",
      "Epoch 51, Train NLL: 2.3878540992736816, Val NLL: 2.407914876937866\n",
      "Epoch 52, Train NLL: 2.383314847946167, Val NLL: 2.4035582542419434\n",
      "Epoch 53, Train NLL: 2.3789095878601074, Val NLL: 2.3993310928344727\n",
      "Epoch 54, Train NLL: 2.374631643295288, Val NLL: 2.3952274322509766\n",
      "Epoch 55, Train NLL: 2.3704755306243896, Val NLL: 2.3912417888641357\n",
      "Epoch 56, Train NLL: 2.366436243057251, Val NLL: 2.3873684406280518\n",
      "Epoch 57, Train NLL: 2.3625078201293945, Val NLL: 2.3836023807525635\n",
      "Epoch 58, Train NLL: 2.3586857318878174, Val NLL: 2.379939317703247\n",
      "Epoch 59, Train NLL: 2.354965925216675, Val NLL: 2.3763747215270996\n",
      "Epoch 60, Train NLL: 2.3513433933258057, Val NLL: 2.372904062271118\n",
      "Epoch 61, Train NLL: 2.3478147983551025, Val NLL: 2.369523525238037\n",
      "Epoch 62, Train NLL: 2.3443758487701416, Val NLL: 2.366229295730591\n",
      "Epoch 63, Train NLL: 2.3410232067108154, Val NLL: 2.36301851272583\n",
      "Epoch 64, Train NLL: 2.3377535343170166, Val NLL: 2.359886646270752\n",
      "Epoch 65, Train NLL: 2.334563732147217, Val NLL: 2.356832265853882\n",
      "Epoch 66, Train NLL: 2.3314504623413086, Val NLL: 2.353850841522217\n",
      "Epoch 67, Train NLL: 2.3284108638763428, Val NLL: 2.3509409427642822\n",
      "Epoch 68, Train NLL: 2.32544207572937, Val NLL: 2.3480987548828125\n",
      "Epoch 69, Train NLL: 2.322542428970337, Val NLL: 2.345322608947754\n",
      "Epoch 70, Train NLL: 2.3197085857391357, Val NLL: 2.342609405517578\n",
      "Epoch 71, Train NLL: 2.3169384002685547, Val NLL: 2.3399579524993896\n",
      "Epoch 72, Train NLL: 2.314229965209961, Val NLL: 2.3373653888702393\n",
      "Epoch 73, Train NLL: 2.3115806579589844, Val NLL: 2.334829568862915\n",
      "Epoch 74, Train NLL: 2.3089890480041504, Val NLL: 2.3323490619659424\n",
      "Epoch 75, Train NLL: 2.306452751159668, Val NLL: 2.3299217224121094\n",
      "Epoch 76, Train NLL: 2.3039700984954834, Val NLL: 2.3275458812713623\n",
      "Epoch 77, Train NLL: 2.301539659500122, Val NLL: 2.3252201080322266\n",
      "Epoch 78, Train NLL: 2.299159526824951, Val NLL: 2.3229422569274902\n",
      "Epoch 79, Train NLL: 2.296828269958496, Val NLL: 2.320711374282837\n",
      "Epoch 80, Train NLL: 2.294543504714966, Val NLL: 2.3185253143310547\n",
      "Epoch 81, Train NLL: 2.2923052310943604, Val NLL: 2.3163838386535645\n",
      "Epoch 82, Train NLL: 2.2901105880737305, Val NLL: 2.314284324645996\n",
      "Epoch 83, Train NLL: 2.287959337234497, Val NLL: 2.3122260570526123\n",
      "Epoch 84, Train NLL: 2.2858495712280273, Val NLL: 2.3102078437805176\n",
      "Epoch 85, Train NLL: 2.283780336380005, Val NLL: 2.3082287311553955\n",
      "Epoch 86, Train NLL: 2.281750440597534, Val NLL: 2.3062872886657715\n",
      "Epoch 87, Train NLL: 2.2797586917877197, Val NLL: 2.304381847381592\n",
      "Epoch 88, Train NLL: 2.277803897857666, Val NLL: 2.3025128841400146\n",
      "Epoch 89, Train NLL: 2.2758851051330566, Val NLL: 2.30067777633667\n",
      "Epoch 90, Train NLL: 2.274001359939575, Val NLL: 2.2988765239715576\n",
      "Epoch 91, Train NLL: 2.2721517086029053, Val NLL: 2.2971079349517822\n",
      "Epoch 92, Train NLL: 2.2703347206115723, Val NLL: 2.2953708171844482\n",
      "Epoch 93, Train NLL: 2.268550157546997, Val NLL: 2.2936649322509766\n",
      "Epoch 94, Train NLL: 2.266796827316284, Val NLL: 2.2919888496398926\n",
      "Epoch 95, Train NLL: 2.265073776245117, Val NLL: 2.290341854095459\n",
      "Epoch 96, Train NLL: 2.263381004333496, Val NLL: 2.288723945617676\n",
      "Epoch 97, Train NLL: 2.261716604232788, Val NLL: 2.2871334552764893\n",
      "Epoch 98, Train NLL: 2.260080337524414, Val NLL: 2.285569906234741\n",
      "Epoch 99, Train NLL: 2.258471727371216, Val NLL: 2.2840325832366943\n",
      "Epoch 100, Train NLL: 2.2568893432617188, Val NLL: 2.2825212478637695\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "trigramW_best = torch.randn((len(btoi), len(stoi)), generator=g, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "reg_param = 0.01\n",
    "lr = 100  \n",
    "\n",
    "for epoch in range(100):\n",
    "    trigramW_best.grad = None\n",
    "\n",
    "    ## Train \n",
    "    logits = X_train_trigram_enc @ trigramW_best\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(y_train_trigram)), y_train_trigram]\n",
    "\n",
    "    trigramNll = -torch.log(pred).mean() + reg_param * (((trigramW_best.data)**2).mean())\n",
    "\n",
    "    trigramNll.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        trigramW_best -= lr*trigramW_best.grad\n",
    "\n",
    "\n",
    "    ## Train and Val losses\n",
    "    with torch.no_grad():\n",
    "        logits = X_train_trigram_enc @ trigramW_best\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_train_trigram)), y_train_trigram]\n",
    "\n",
    "        trigramNll_train = -torch.log(pred).mean().item()\n",
    "\n",
    "        logits = X_val_trigram_enc @ trigramW_best\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "        pred = probs[torch.arange(0, len(y_val_trigram)), y_val_trigram]\n",
    "\n",
    "        trigramNll_val = -torch.log(pred).mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train NLL: {trigramNll_train}, Val NLL: {trigramNll_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Get the test NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2652206420898438"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = X_test_trigram_enc @ trigramW_best\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / (counts.sum(dim=1, keepdim=True))\n",
    "\n",
    "    pred = probs[torch.arange(0, len(y_test_trigram)), y_test_trigram]\n",
    "\n",
    "    trigramNll_test = -torch.log(pred).mean().item()\n",
    "\n",
    "trigramNll_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
