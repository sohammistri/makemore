{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop ninja from scratch\n",
    "\n",
    "Implement makemore part 4 (comment `loss.backward()` and write backprop stepwise) from an empty  notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../names.txt', 'r') as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(\"\".join(name for name in names))))\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi, itos = {}, {}\n",
    "\n",
    "stoi['.'] = 0\n",
    "itos[0] = '.'\n",
    "\n",
    "for i, ch in enumerate(chars):\n",
    "    stoi[ch] = i + 1\n",
    "    itos[i + 1] = ch\n",
    "\n",
    "len(stoi), len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3204)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Split the dataset into train and temp sets (80% train, 20% temp)\n",
    "train_names, temp_names = train_test_split(names, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Split the temp set into validation and test sets (50% val, 50% test of the 20% temp set)\n",
    "val_names, test_names = train_test_split(temp_names, test_size=0.5, random_state=random_seed)\n",
    "\n",
    "len(train_names), len(val_names), len(test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_dataset(names, block_size):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name in names:\n",
    "        name = ['.'] * block_size + list(name) + ['.']\n",
    "        for i, ch in enumerate(name[block_size:]):\n",
    "            input_seq, output_char = name[i:i+block_size], ch\n",
    "            X.append([stoi[c] for c in input_seq])\n",
    "            y.append(stoi[output_char])\n",
    "\n",
    "    X = torch.LongTensor(X)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182497, 3]),\n",
       " torch.Size([182497]),\n",
       " torch.Size([22882, 3]),\n",
       " torch.Size([22882]),\n",
       " torch.Size([22767, 3]),\n",
       " torch.Size([22767]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3\n",
    "X_train, y_train = create_dataset(train_names, block_size)\n",
    "X_val, y_val = create_dataset(val_names, block_size)\n",
    "X_test, y_test = create_dataset(test_names, block_size)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop ninja exercise 1: Atomic portions of the backprop\n",
    "\n",
    "We split every small portion of the model and compute gradients of the same\n",
    "\n",
    "## Learnings:\n",
    "- Always use `clone` when just copying a tensor.\n",
    "- Be careful about backproping in maximum\n",
    "- Avoid dividing instead use ** or simple multiply by reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24297\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "emb_dim = 10\n",
    "hidden_dim = 400\n",
    "vocab_size = len(stoi)\n",
    "block_size = 3\n",
    "\n",
    "# emb matrix\n",
    "C = torch.randn((vocab_size, emb_dim), dtype=torch.float32, generator=g)\n",
    "\n",
    "# 1st linear layer\n",
    "W1 = torch.randn((emb_dim * block_size, hidden_dim), dtype=torch.float32, generator=g) * 1/((emb_dim * block_size)**0.50)\n",
    "b1 = torch.randn(hidden_dim, dtype=torch.float32, generator=g) * 0.10 # not really needed due to BN so expect the grads to be very small\n",
    "\n",
    "# BatchNorm layer\n",
    "bngain = torch.randn((1, hidden_dim), dtype=torch.float32, generator=g)\n",
    "bnbias = torch.randn((1, hidden_dim), dtype=torch.float32, generator=g)\n",
    "\n",
    "# 2nd linear layer\n",
    "W2 = torch.randn((hidden_dim, vocab_size), dtype=torch.float32, generator=g) * 1/((hidden_dim)**0.50) * 0.10\n",
    "b2 = torch.randn(vocab_size, dtype=torch.float32, generator=g) * 0.10\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2864649295806885"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass step by step\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "idx = torch.randint(0, len(X_train), (batch_size,), generator=g)\n",
    "Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "# embedding indexing\n",
    "emb = C[Xb]\n",
    "\n",
    "# 1st linear layer\n",
    "embcat = emb.view(batch_size, -1)\n",
    "h1 = embcat @ W1 + b1\n",
    "\n",
    "# Batchnorm layer\n",
    "bnmean = (1/batch_size) * torch.sum(h1, dim=0, keepdim=True)\n",
    "bndiff = h1 - bnmean\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = (1/(batch_size - 1)) * torch.sum(bndiff2, dim=0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hbn = bnraw * bngain + bnbias\n",
    "\n",
    "# Tanh layer\n",
    "hact = torch.tanh(hbn)\n",
    "\n",
    "# 2nd linear layer\n",
    "logits = hact @ W2 + b2\n",
    "\n",
    "# Loss computation\n",
    "logits_max = logits.max(dim=1, keepdim=True).values\n",
    "norm_logits = logits - logits_max\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "probs_idx = probs[range(batch_size), yb]\n",
    "log_probs = probs_idx.log()\n",
    "loss = -log_probs.mean()\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [emb, embcat, h1, bnmean, bndiff, bndiff2, bnvar, bnvar_inv, bnraw, hbn, hact,\\\n",
    "          logits, logits_max, norm_logits, counts, counts_sum, counts_sum_inv, probs, probs_idx, log_probs]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(tensor_name, t, dt):\n",
    "    exact = str(torch.all(t.grad == dt).item())\n",
    "    approx = str(torch.torch.allclose(t.grad, dt))\n",
    "    max_diff = (t.grad - dt).abs().max().item()\n",
    "    print(f\"{tensor_name:35s} | exact: {exact:5s} | approximate: {approx:5s} | max difference:{max_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs       | exact: True  | approximate: True  | max difference:0.0\n",
      "probs_idx       | exact: True  | approximate: True  | max difference:0.0\n",
      "probs           | exact: True  | approximate: True  | max difference:0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | max difference:0.0\n",
      "counts_sum      | exact: True  | approximate: True  | max difference:0.0\n",
      "counts          | exact: True  | approximate: True  | max difference:0.0\n",
      "norm_logits     | exact: True  | approximate: True  | max difference:0.0\n",
      "logits_max      | exact: True  | approximate: True  | max difference:0.0\n",
      "logits          | exact: True  | approximate: True  | max difference:0.0\n",
      "hact            | exact: True  | approximate: True  | max difference:0.0\n",
      "W2              | exact: True  | approximate: True  | max difference:0.0\n",
      "b2              | exact: True  | approximate: True  | max difference:0.0\n",
      "hbn             | exact: True  | approximate: True  | max difference:0.0\n",
      "bngain          | exact: True  | approximate: True  | max difference:0.0\n",
      "bnbias          | exact: True  | approximate: True  | max difference:0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | max difference:0.0\n",
      "bnvar           | exact: True  | approximate: True  | max difference:0.0\n",
      "bndiff2         | exact: True  | approximate: True  | max difference:0.0\n",
      "bndiff          | exact: True  | approximate: True  | max difference:0.0\n",
      "bnmean          | exact: True  | approximate: True  | max difference:0.0\n",
      "h1              | exact: True  | approximate: True  | max difference:0.0\n",
      "embcat          | exact: True  | approximate: True  | max difference:0.0\n",
      "W1              | exact: True  | approximate: True  | max difference:0.0\n",
      "b1              | exact: True  | approximate: True  | max difference:0.0\n",
      "emb             | exact: True  | approximate: True  | max difference:0.0\n",
      "C               | exact: True  | approximate: True  | max difference:0.0\n"
     ]
    }
   ],
   "source": [
    "# line 39 loss = log_probs.mean() log_probs shape : [B, 1]\n",
    "dlog_probs = -1/batch_size * torch.ones_like(log_probs)\n",
    "\n",
    "# line 38 log_probs = probs_idx.log() probs_idx shape : [B, 1]\n",
    "dprobs_idx = (1 / probs_idx) * dlog_probs\n",
    "\n",
    "# line 37 probs_idx = probs[range(batch_size), yb] probs shape : [B, V]\n",
    "dprobs = torch.zeros_like(probs)\n",
    "dprobs[range(batch_size), yb] = dprobs_idx\n",
    "\n",
    "# line 36 probs = counts * counts_sum_inv , counts shape : [B ,V], counts_sum_inv : [B, 1]\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "# line 35 counts_sum_inv = counts_sum**-1 counts_sum shape : [B, 1]\n",
    "dcounts_sum = -(counts_sum**-2) * dcounts_sum_inv\n",
    "\n",
    "# line 34 counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# line 33 counts = norm_logits.exp() norm_logits shape : [B, V]\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "\n",
    "# line 32 norm_logits = logits - logits_max , logits shape : [B, V], logits_max : [B, 1]\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits_max = (-dnorm_logits).sum(dim=1, keepdim=True)\n",
    "\n",
    "# line 31 logits_max = logits.max(dim=1, keepdim=True).values\n",
    "## CAREFUL for max backprop\n",
    "max_indices = logits.max(dim=1, keepdim=True).indices.view(-1)\n",
    "dlogits[range(batch_size), max_indices] += dlogits_max[range(batch_size), 0]\n",
    "\n",
    "# line 28 logits = hact @ W2 + b2, hact shape: [B, H], W2 shape: [H, V], b2 shape: [V]\n",
    "dhact = dlogits @ W2.T\n",
    "dW2 = hact.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0, keepdim=True).view(-1)\n",
    "\n",
    "# line 25 hact = torch.tanh(hbn) hbn shape: [B, H]\n",
    "dhbn = (1 - hact**2) * dhact\n",
    "\n",
    "# line 22 hbn = bnraw * bngain + bnbias bnraw shape: [B, H], bngain shape: [1, H], bnbias shape: [1, H]\n",
    "dbnraw = bngain * dhbn\n",
    "dbngain = (bnraw * dhbn).sum(dim=0, keepdim=True)\n",
    "dbnbias = (dhbn).sum(dim=0, keepdim=True)\n",
    "\n",
    "# line 21 bnraw = bndiff * bnvar_inv bndiff shape: [B, H], bnvar_inv shape: [1, H]\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(dim=0, keepdim=True)\n",
    "\n",
    "# line 20 bnvar_inv = (bnvar + 1e-5)**-0.5 bnvar shape: [1, H]\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "\n",
    "# line 19 bnvar = (1/(batch_size - 1)) * torch.sum(bndiff2, dim=0, keepdim=True) bndiff2 shape: [B, H]\n",
    "dbndiff2 = (torch.ones_like(bndiff2) * dbnvar) * (1 / (batch_size - 1)) # prefer to multiply 1/n instead of dividing by n\n",
    "\n",
    "# line 18 bndiff2 = bndiff**2\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "\n",
    "# line 17 bndiff = h1 - bnmean h1 shape: [B, H], bnmean shape: [1, H]\n",
    "dh1 = dbndiff.clone()\n",
    "dbnmean = (-dbndiff).sum(dim=0, keepdim=True)\n",
    "\n",
    "# line 16 bnmean = (1/batch_size) * torch.sum(h1, dim=0, keepdim=True)\n",
    "dh1 += torch.ones_like(h1) * (1/batch_size) * dbnmean\n",
    "\n",
    "# line 13 h1 = embcat @ W1 + b1 embcat shape: [B, BS*E], W1 shape: [BS*E, H], b1 shape: [H]\n",
    "dembcat = dh1 @ W1.T\n",
    "dW1 = embcat.T @ dh1\n",
    "db1 = dh1.sum(dim=0, keepdim=True).view(-1)\n",
    "\n",
    "# line 12 embcat = emb.view(batch_size, -1) emb shape: [B, BS, E]\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "# line 9 emb = C[Xb] C shape: [V, E], Xb shape: [B, BS]\n",
    "dC = torch.zeros_like(C)\n",
    "dC.index_add_(dim=0, index = Xb.view(-1), source=demb.view(-1, emb_dim));\n",
    "\n",
    "cmp(\"log_probs\", log_probs, dlog_probs)\n",
    "cmp(\"probs_idx\", probs_idx, dprobs_idx)\n",
    "cmp(\"probs\", probs, dprobs)\n",
    "cmp(\"counts_sum_inv\", counts_sum_inv, dcounts_sum_inv)\n",
    "cmp(\"counts_sum\", counts_sum, dcounts_sum)\n",
    "cmp(\"counts\", counts, dcounts)\n",
    "cmp(\"norm_logits\", norm_logits, dnorm_logits)\n",
    "cmp(\"logits_max\", logits_max, dlogits_max)\n",
    "cmp(\"logits\", logits, dlogits)\n",
    "cmp(\"hact\", hact, dhact)\n",
    "cmp(\"W2\", W2, dW2)\n",
    "cmp(\"b2\", b2, db2)\n",
    "cmp(\"hbn\", hbn, dhbn)\n",
    "cmp(\"bngain\", bngain, dbngain)\n",
    "cmp(\"bnbias\", bnbias, dbnbias)\n",
    "cmp(\"bnvar_inv\", bnvar_inv, dbnvar_inv)\n",
    "cmp(\"bnvar\", bnvar, dbnvar)\n",
    "cmp(\"bndiff2\", bndiff2, dbndiff2)\n",
    "cmp(\"bndiff\", bndiff, dbndiff)\n",
    "cmp(\"bnmean\", bnmean, dbnmean)\n",
    "cmp(\"h1\", h1, dh1)\n",
    "cmp(\"embcat\", embcat, dembcat)\n",
    "cmp(\"W1\", W1, dW1)\n",
    "cmp(\"b1\", b1, db1)\n",
    "cmp(\"emb\", emb, demb)\n",
    "cmp(\"C\", C, dC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop ninja exercise 2: directly backprop into the CE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss | exact: True  | approximate: True  | max difference:0.0\n"
     ]
    }
   ],
   "source": [
    "loss_ce = torch.nn.functional.cross_entropy(logits, yb)\n",
    "exact = str(torch.all(loss == loss_ce).item())\n",
    "approx = str(torch.torch.allclose(loss, loss_ce))\n",
    "max_diff = (loss - loss_ce).abs().max().item()\n",
    "print(f\"Loss | exact: {exact:5s} | approximate: {approx:5s} | max difference:{max_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | max difference:2.0081643015146255e-09\n"
     ]
    }
   ],
   "source": [
    "dlogits = None # logits shape [B, V]\n",
    "dlogits = torch.softmax(logits, dim=1)\n",
    "dlogits[range(batch_size), yb] -= 1.0\n",
    "dlogits *= (1/batch_size)\n",
    "cmp(\"logits\", logits, dlogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop ninja exercise 3: directly backprop into BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbn_direct = bngain * ((h1 - h1.mean(dim=0, keepdim=True)) / torch.sqrt(h1.var(dim=0, keepdim=True, unbiased=True) + 1e-5)) + bnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm | exact: False | approximate: False | max difference:9.5367431640625e-07\n"
     ]
    }
   ],
   "source": [
    "exact = str(torch.all(hbn == hbn_direct).item())\n",
    "approx = str(torch.torch.allclose(hbn, hbn_direct))\n",
    "max_diff = (hbn - hbn_direct).abs().max().item()\n",
    "print(f\"BatchNorm | exact: {exact:5s} | approximate: {approx:5s} | max difference:{max_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1              | exact: False | approximate: True  | max difference:2.9103830456733704e-11\n"
     ]
    }
   ],
   "source": [
    "dh1 = None\n",
    "dh1 = ((bngain * bnvar_inv) / batch_size) * (batch_size * dhbn -\\\n",
    "                                                         (batch_size / (batch_size - 1)) * bnraw * torch.sum(bnraw * dhbn, dim=0, keepdim=True) -\\\n",
    "                                                            torch.sum(dhbn, dim=0, keepdim=True))\n",
    "cmp(\"h1\", h1, dh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop ninja exercise 4: Bring all this together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use loss.backward (small doge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24297\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "emb_dim = 10\n",
    "hidden_dim = 400\n",
    "vocab_size = len(stoi)\n",
    "block_size = 3\n",
    "\n",
    "# emb matrix\n",
    "C = torch.randn((vocab_size, emb_dim), dtype=torch.float32, generator=g)\n",
    "\n",
    "# 1st linear layer\n",
    "W1 = torch.randn((emb_dim * block_size, hidden_dim), dtype=torch.float32, generator=g) * 1/((emb_dim * block_size)**0.50)\n",
    "b1 = torch.randn(hidden_dim, dtype=torch.float32, generator=g) * 0.10 # not really needed due to BN so expect the grads to be very small\n",
    "\n",
    "# BatchNorm layer\n",
    "bngain = torch.randn((1, hidden_dim), dtype=torch.float32, generator=g)\n",
    "bnbias = torch.randn((1, hidden_dim), dtype=torch.float32, generator=g)\n",
    "\n",
    "# 2nd linear layer\n",
    "W2 = torch.randn((hidden_dim, vocab_size), dtype=torch.float32, generator=g) * 1/((hidden_dim)**0.50) * 0.10\n",
    "b2 = torch.randn(vocab_size, dtype=torch.float32, generator=g) * 0.10\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_iters = 200000\n",
    "lr_scheduler = lambda i: 10**(-0.95) if i<100000 else 10**(-1.95)\n",
    "epsilon = 1e-5\n",
    "momentum = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/200000 ---> 3.277965784072876\n",
      "Step 10000/200000 ---> 2.2029948234558105\n",
      "Step 20000/200000 ---> 1.9918242692947388\n",
      "Step 30000/200000 ---> 1.9321165084838867\n",
      "Step 40000/200000 ---> 2.196711778640747\n",
      "Step 50000/200000 ---> 2.333263397216797\n",
      "Step 60000/200000 ---> 2.2127952575683594\n",
      "Step 70000/200000 ---> 2.121904134750366\n",
      "Step 80000/200000 ---> 1.9377796649932861\n",
      "Step 90000/200000 ---> 1.9899767637252808\n",
      "Step 100000/200000 ---> 1.9825598001480103\n",
      "Step 110000/200000 ---> 2.0687739849090576\n",
      "Step 120000/200000 ---> 1.9552937746047974\n",
      "Step 130000/200000 ---> 2.260519027709961\n",
      "Step 140000/200000 ---> 2.088731050491333\n",
      "Step 150000/200000 ---> 2.017077922821045\n",
      "Step 160000/200000 ---> 1.8850016593933105\n",
      "Step 170000/200000 ---> 1.688708782196045\n",
      "Step 180000/200000 ---> 2.0358781814575195\n",
      "Step 190000/200000 ---> 1.9506179094314575\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "stepi, lossi = [], []\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "running_mean, running_var = torch.zeros((1, hidden_dim)).float(), torch.ones((1, hidden_dim)).float()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    idx = torch.randint(0, len(X_train), (batch_size,), generator=g)\n",
    "    Xb, yb = X_train[idx], y_train[idx]\n",
    "\n",
    "    # embedding\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(batch_size, -1)\n",
    "\n",
    "    # linear layer 1\n",
    "    h = embcat @ W1 + b1\n",
    "\n",
    "    # batchnorm\n",
    "    bnmean = torch.mean(h, dim=0, keepdim=True)\n",
    "    bnvar = torch.var(h, dim=0, keepdim=True)\n",
    "    bnraw = (h - bnmean) / torch.sqrt(bnvar + epsilon)\n",
    "    hbn = bnraw * bngain + bnbias\n",
    "    # update running stats\n",
    "    with torch.no_grad():\n",
    "        running_mean = momentum * bnmean + (1 - momentum) * running_mean\n",
    "        running_var = momentum * bnvar + (1 - momentum) * running_var\n",
    "\n",
    "    # tanh nonlinearity\n",
    "    hact = torch.tanh(hbn)\n",
    "\n",
    "    # linear layer 2\n",
    "    logits = hact @ W2 + b2\n",
    "\n",
    "    # compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits, yb)\n",
    "\n",
    "    # set param grads to None, backward and update params\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p.data -= lr_scheduler(i) * p.grad\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Step {i}/{n_iters} ---> {loss.item()}\")\n",
    "\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x137716390>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_loss(X, y):\n",
    "    # forward\n",
    "    emb = C[X]\n",
    "    embcat = emb.view(len(X), -1)\n",
    "    h = embcat @ W1 + b1\n",
    "    bnraw = (h - running_mean) / torch.sqrt(running_var + epsilon)\n",
    "    hbn = bnraw * bngain + bnbias\n",
    "    hact = torch.tanh(hbn)\n",
    "    logits = hact @ W2 + b2\n",
    "\n",
    "    # compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.00002121925354, 2.0803074836730957, 2.094284772872925)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = compute_loss(X_train, y_train)\n",
    "val_loss = compute_loss(X_val, y_val)\n",
    "test_loss = compute_loss(X_test, y_test)\n",
    "train_loss, val_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write own gradients (swoll doge :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24297\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "emb_dim = 10\n",
    "hidden_dim = 400\n",
    "vocab_size = len(stoi)\n",
    "block_size = 3\n",
    "\n",
    "# emb matrix\n",
    "C = torch.randn((vocab_size, emb_dim), dtype=torch.float32, generator=g)\n",
    "\n",
    "# 1st linear layer\n",
    "W1 = torch.randn((emb_dim * block_size, hidden_dim), dtype=torch.float32, generator=g) * 1/((emb_dim * block_size)**0.50)\n",
    "b1 = torch.randn(hidden_dim, dtype=torch.float32, generator=g) * 0.10 # not really needed due to BN so expect the grads to be very small\n",
    "\n",
    "# BatchNorm layer\n",
    "bngain = torch.randn((1, hidden_dim), dtype=torch.float32, generator=g)\n",
    "bnbias = torch.randn((1, hidden_dim), dtype=torch.float32, generator=g)\n",
    "\n",
    "# 2nd linear layer\n",
    "W2 = torch.randn((hidden_dim, vocab_size), dtype=torch.float32, generator=g) * 1/((hidden_dim)**0.50) * 0.10\n",
    "b2 = torch.randn(vocab_size, dtype=torch.float32, generator=g) * 0.10\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_iters = 200000\n",
    "lr_scheduler = lambda i: 10**(-0.95) if i<100000 else 10**(-1.95)\n",
    "epsilon = 1e-5\n",
    "momentum = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/200000 ---> 3.277965784072876\n",
      "Step 10000/200000 ---> 2.2029950618743896\n",
      "Step 20000/200000 ---> 1.9918245077133179\n",
      "Step 30000/200000 ---> 1.9321160316467285\n",
      "Step 40000/200000 ---> 2.196713447570801\n",
      "Step 50000/200000 ---> 2.333263397216797\n",
      "Step 60000/200000 ---> 2.212798595428467\n",
      "Step 70000/200000 ---> 2.121904134750366\n",
      "Step 80000/200000 ---> 1.937780737876892\n",
      "Step 90000/200000 ---> 1.9899792671203613\n",
      "Step 100000/200000 ---> 1.9825620651245117\n",
      "Step 110000/200000 ---> 2.068772792816162\n",
      "Step 120000/200000 ---> 1.9552940130233765\n",
      "Step 130000/200000 ---> 2.2605247497558594\n",
      "Step 140000/200000 ---> 2.0887339115142822\n",
      "Step 150000/200000 ---> 2.0170822143554688\n",
      "Step 160000/200000 ---> 1.8849965333938599\n",
      "Step 170000/200000 ---> 1.688709020614624\n",
      "Step 180000/200000 ---> 2.0358822345733643\n",
      "Step 190000/200000 ---> 1.950619101524353\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "stepi, lossi = [], []\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "running_mean, running_var = torch.zeros((1, hidden_dim)).float(), torch.ones((1, hidden_dim)).float()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    ix = torch.randint(low=0, high=X_train.shape[0], size=(batch_size,), generator=g)\n",
    "    Xb, yb = X_train[ix], y_train[ix] # batch X,Y\n",
    "\n",
    "    # embedding\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(batch_size, -1)\n",
    "\n",
    "    # linear layer 1\n",
    "    h = embcat @ W1 + b1\n",
    "\n",
    "    # batchnorm\n",
    "    bnmean = torch.mean(h, dim=0, keepdim=True)\n",
    "    bnvar = torch.var(h, dim=0, keepdim=True)\n",
    "    bnvar_inv = (bnvar + epsilon) ** (-0.50)\n",
    "    bnraw = (h - bnmean) * bnvar_inv\n",
    "    hbn = bnraw * bngain + bnbias\n",
    "    # update running stats\n",
    "    with torch.no_grad():\n",
    "        running_mean = momentum * bnmean + (1 - momentum) * running_mean\n",
    "        running_var = momentum * bnvar + (1 - momentum) * running_var\n",
    "\n",
    "    # tanh nonlinearity\n",
    "    hact = torch.tanh(hbn)\n",
    "\n",
    "    # linear layer 2\n",
    "    logits = hact @ W2 + b2\n",
    "\n",
    "    # compute loss\n",
    "    loss = torch.nn.functional.cross_entropy(logits, yb)\n",
    "\n",
    "    ## DEBUG: will remove all this till backward!!\n",
    "    # set param grads to None, backward and update params\n",
    "    # for p in parameters:\n",
    "    #     p.grad = None\n",
    "\n",
    "    # loss.backward()\n",
    "\n",
    "    dC, dW1, db1, dbngain, dbnbias, dW2, db2 = None, None, None, None, None, None, None\n",
    "\n",
    "    # write own grads :)\n",
    "    with torch.no_grad():\n",
    "        dlogits = torch.softmax(logits, dim=1)\n",
    "        dlogits[range(batch_size), yb] -= 1.0\n",
    "        dlogits *= (1/batch_size)\n",
    "        \n",
    "        dW2 = hact.T @ dlogits\n",
    "        db2 = dlogits.sum(dim=0, keepdim=True).view(-1)\n",
    "        dhact = dlogits @ W2.T\n",
    "\n",
    "        dhbn = (1 - hact**2) * dhact\n",
    "\n",
    "        dbngain = (bnraw * dhbn).sum(dim=0, keepdim=True)\n",
    "        dbnbias = (dhbn).sum(dim=0, keepdim=True)\n",
    "\n",
    "        dh = ((bngain * bnvar_inv) / batch_size) * (batch_size * dhbn -\\\n",
    "                                                         (batch_size / (batch_size - 1)) * bnraw * torch.sum(bnraw * dhbn, dim=0, keepdim=True) -\\\n",
    "                                                            torch.sum(dhbn, dim=0, keepdim=True))\n",
    "        \n",
    "        dW1 = embcat.T @ dh\n",
    "        db1 = dh.sum(dim=0, keepdim=True).view(-1)\n",
    "        dembcat = dh @ W1.T\n",
    "\n",
    "        demb = dembcat.view(emb.shape)\n",
    "\n",
    "        dC = torch.zeros_like(C)\n",
    "        dC.index_add_(0, Xb.view(-1), demb.view(-1, emb_dim));\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # print(p.shape, g.shape)\n",
    "            p.data -= lr_scheduler(i) * grad.data\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Step {i}/{n_iters} ---> {loss.item()}\")\n",
    "\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.00002121925354, 2.0803072452545166, 2.0942845344543457)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = compute_loss(X_train, y_train)\n",
    "val_loss = compute_loss(X_val, y_val)\n",
    "test_loss = compute_loss(X_test, y_test)\n",
    "train_loss, val_loss, test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
