model:
  name: "Sequential-Character-Level-Language-Model-Pretrain"
  ckpt_name: "lstm_pretrain_best_model_seq_128"
  architecture:
    type: "lstm"
    num_layers: 2
    embedding_dim: 256
    hidden_dim: 256
    dropout: 0.0
    batch_first: True

training:
  file_path: "/root/makemore/personal-extension/character_level_lm/pretrain_data/pretrain_data_seq_len_128.npz"
  batch_size: 1024
  sequence_length: 128
  epochs: 5
  learning_rate: 0.01

hardware:
  device: "cuda"